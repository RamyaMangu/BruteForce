{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "97d9baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('https://query.data.world/s/5xzuftmozqteqbqzopbcgxbjzusyi7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e72b9401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the training set and test set with test_size = 0.4\n",
    "def split_train_test(df):\n",
    "    train_set, test_set = train_test_split(df, test_size=0.4, random_state=42)\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e0a12bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reomve pattern function, input_text is the text we want to process, the pattern is the pattern we want to remove\n",
    "def remove_pattern(input_text, pattern):\n",
    "    r = re.findall(pattern, input_text)\n",
    "    for i in r:\n",
    "        input_text = re.sub(i, '', input_text)\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1918762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "06fb389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the data\n",
    "def preprocess(df):\n",
    "    train_set,test_set = split_train_test(df)\n",
    "    #combine the train_set and test_set to preprocess the data\n",
    "    combi = pd.concat([train_set,test_set], ignore_index=True)\n",
    "    \n",
    "    #remove the @\n",
    "    combi['processed'] = np.vectorize(remove_pattern)(combi['content'], \"@[\\w]*\")\n",
    "    \n",
    "    #remove the #\n",
    "    combi['processed'] = np.vectorize(remove_pattern)(combi['processed'], \"#\")\n",
    "    \n",
    "    #remove emoji\n",
    "    combi['processed'] = np.vectorize(remove_pattern)(combi['processed'], emoji)\n",
    "    \n",
    "    #remove the special characters, numbers, punctuations\n",
    "    combi['processed'] = combi['processed'].str.replace(\"[^a-zA-Z#]\", \" \",regex=True)\n",
    "    \n",
    "     #remove the words smaller than 2\n",
    "    combi['processed'] = combi['processed'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "    \n",
    "    return combi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a1f97d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_sentiment(combi):\n",
    "    drop_index = combi[(combi['sentiment']=='empty') | (combi['sentiment']=='enthusiasm') | (combi['sentiment']=='fun')].index\n",
    "    combi = combi.drop(drop_index)\n",
    "    return combi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4a3c2a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load csv file\n",
    "df = pd.read_csv('https://query.data.world/s/5xzuftmozqteqbqzopbcgxbjzusyi7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4d82dfe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1694088996</td>\n",
       "      <td>love</td>\n",
       "      <td>whyamievenhere</td>\n",
       "      <td>@zoeatthedisco lol hell yes i'm keen. WE'RE GO...</td>\n",
       "      <td>lol hell yes keen GOING SKIING TREBLE CONE SOM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1751715234</td>\n",
       "      <td>love</td>\n",
       "      <td>theamericanxp</td>\n",
       "      <td>Feeling special @ looking4him first guy to giv...</td>\n",
       "      <td>Feeling special looking him first guy give flo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1694272881</td>\n",
       "      <td>neutral</td>\n",
       "      <td>akane_takamura</td>\n",
       "      <td>?and make your own pledge while you're at it!</td>\n",
       "      <td>and make your own pledge while you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1753094560</td>\n",
       "      <td>worry</td>\n",
       "      <td>CVJason</td>\n",
       "      <td>@mosdefaqueen My pleasure. I can't be with my ...</td>\n",
       "      <td>pleasure can with mom Mother Day But can sprea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1961438666</td>\n",
       "      <td>worry</td>\n",
       "      <td>santoleto</td>\n",
       "      <td>6:29 pm - ok, let's go now through #bowman #st...</td>\n",
       "      <td>let now through bowman strategicClock but firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>1957212416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>summatusmentis</td>\n",
       "      <td>really, realy want a netbook. mrr. Techno-lust...</td>\n",
       "      <td>really realy want netbook mrr Techno lust sucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>1751921888</td>\n",
       "      <td>happiness</td>\n",
       "      <td>GaByDiAz</td>\n",
       "      <td>@ Butlers watching Dr. Farmer rock out w/ the ...</td>\n",
       "      <td>Butlers watching Farmer rock out the hispanic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>1695220613</td>\n",
       "      <td>surprise</td>\n",
       "      <td>jesssicababesss</td>\n",
       "      <td>@iyaitssuzanne ohh yeh , but he was on sexy me...</td>\n",
       "      <td>ohh yeh but was sexy men its okay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>1963521553</td>\n",
       "      <td>worry</td>\n",
       "      <td>jerrybruno</td>\n",
       "      <td>Just found about ten typos in one story in the...</td>\n",
       "      <td>Just found about ten typos one story the Plain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>1966072241</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neessssssa</td>\n",
       "      <td>HEY GUYS IT'S WORKING NO NEED TO WORRY. i have...</td>\n",
       "      <td>HEY GUYS WORKING NEED WORRY have tooo many fol...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36638 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id  sentiment           author  \\\n",
       "0      1694088996       love   whyamievenhere   \n",
       "1      1751715234       love    theamericanxp   \n",
       "3      1694272881    neutral   akane_takamura   \n",
       "4      1753094560      worry          CVJason   \n",
       "5      1961438666      worry        santoleto   \n",
       "...           ...        ...              ...   \n",
       "39995  1957212416    neutral   summatusmentis   \n",
       "39996  1751921888  happiness         GaByDiAz   \n",
       "39997  1695220613   surprise  jesssicababesss   \n",
       "39998  1963521553      worry       jerrybruno   \n",
       "39999  1966072241    neutral       neessssssa   \n",
       "\n",
       "                                                 content  \\\n",
       "0      @zoeatthedisco lol hell yes i'm keen. WE'RE GO...   \n",
       "1      Feeling special @ looking4him first guy to giv...   \n",
       "3          ?and make your own pledge while you're at it!   \n",
       "4      @mosdefaqueen My pleasure. I can't be with my ...   \n",
       "5      6:29 pm - ok, let's go now through #bowman #st...   \n",
       "...                                                  ...   \n",
       "39995  really, realy want a netbook. mrr. Techno-lust...   \n",
       "39996  @ Butlers watching Dr. Farmer rock out w/ the ...   \n",
       "39997  @iyaitssuzanne ohh yeh , but he was on sexy me...   \n",
       "39998  Just found about ten typos in one story in the...   \n",
       "39999  HEY GUYS IT'S WORKING NO NEED TO WORRY. i have...   \n",
       "\n",
       "                                               processed  \n",
       "0      lol hell yes keen GOING SKIING TREBLE CONE SOM...  \n",
       "1      Feeling special looking him first guy give flo...  \n",
       "3                     and make your own pledge while you  \n",
       "4      pleasure can with mom Mother Day But can sprea...  \n",
       "5      let now through bowman strategicClock but firs...  \n",
       "...                                                  ...  \n",
       "39995    really realy want netbook mrr Techno lust sucks  \n",
       "39996  Butlers watching Farmer rock out the hispanic ...  \n",
       "39997                  ohh yeh but was sexy men its okay  \n",
       "39998  Just found about ten typos one story the Plain...  \n",
       "39999  HEY GUYS WORKING NEED WORRY have tooo many fol...  \n",
       "\n",
       "[36638 rows x 5 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed = preprocess(df)\n",
    "df_processed = drop_sentiment(df_processed)\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c1d21985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love',\n",
       " 'neutral',\n",
       " 'worry',\n",
       " 'happiness',\n",
       " 'hate',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'relief',\n",
       " 'boredom',\n",
       " 'anger']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The sentiment list\n",
    "senti_list = df_processed['sentiment'].unique().tolist()\n",
    "senti_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "730fee8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\11580\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1b9052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "eb76eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using nltk to get the CountVectorizer and frequency dict of given sentiment\n",
    "def word_token(df_processed, sentiment):\n",
    "    text = []\n",
    "    freq_text = []\n",
    "    stop = set(stopwords.words('english'))\n",
    "    import nltk\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    for str in df_processed[(df_processed['sentiment']==sentiment)]['processed']:\n",
    "        freq_text.extend([x.lower() for x in str.split(' ') if x not in stop])\n",
    "        text.append(str.lower())\n",
    "\n",
    "    cv = CountVectorizer(max_df=0.9, min_df=2, stop_words='english')\n",
    "    cv.fit_transform(text)\n",
    "    from nltk import FreqDist\n",
    "    fdist = FreqDist(freq_text)\n",
    "    sorted(fdist.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    return cv, sorted(fdist.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1c2882c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get love cv and fdist\n",
    "cv_love, fdist_love = word_token(df_processed, 'love')\n",
    "#get other sentiments' cv and fdist\n",
    "cv_neutral, fdist_neutral = word_token(df_processed, 'neutral')\n",
    "cv_worry, fdist_worry = word_token(df_processed, 'worry')\n",
    "cv_happy, fdist_happy = word_token(df_processed, 'happiness')\n",
    "cv_hate, fdist_hate = word_token(df_processed, 'hate')\n",
    "cv_sadness, fdist_sadness = word_token(df_processed, 'sadness')\n",
    "cv_surprise, fdist_surprise = word_token(df_processed, 'surprise')\n",
    "cv_relief, fdist_relief = word_token(df_processed, 'relief')\n",
    "cv_born, fdist_born = word_token(df_processed, 'boredom')\n",
    "cv_anger, fdist_anger = word_token(df_processed, 'anger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "892ce530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from pandas import Series,DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "#Put your Bearer Token in the parenthesis below\n",
    "client = tweepy.Client(bearer_token='AAAAAAAAAAAAAAAAAAAAAHiaigEAAAAA1hzpI79DUjAi9q8PvGD7lfTzWjQ%3Dw9PNTdJGBWQOWxJjY9l5yP1Z7fglAF5SQGbLN6LrzUHQ5Gvbkd')\n",
    "\n",
    "# Get tweets that contain the hashtag #petday\n",
    "# -is:retweet means I don't want retweets\n",
    "# lang:en is asking for the tweets to be in english\n",
    "query = 'covid -is:retweet'\n",
    "tweets = client.search_recent_tweets(query = query, tweet_fields=['context_annotations', 'created_at'], max_results=100)\n",
    "\n",
    "tweet_lst = [tweet.text for tweet in tweets.data]\n",
    "\n",
    "\n",
    "\n",
    "data = {'content': tweet_lst}\n",
    "tweet_df = DataFrame(data)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e8538093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A grande mÃ­dia estÃ¡ inventando um \"suposto\" su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@poetWOAgun How much proof does America AND th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wujudkan Masyarakat Yang Sehat dan Produktif D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://t.co/2vo3iF4r2h The Unforgivable Reque...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@MariaBiddle9 @LeahLschkupz @BoSnerdley Well t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>@DollyD2022 The covid Vax causes spires in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>âœ… Moderate Republican \\nâœ… Female\\nâœ… White\\nâœ… P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>@Den_1er Ni pour Poutine ni pour Zelensky et i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>@CovergirlChase @frandrescher @RockmondDunbar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Anggaran untuk perbaikan RPTRA tahun ini sudah...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content\n",
       "0   A grande mÃ­dia estÃ¡ inventando um \"suposto\" su...\n",
       "1   @poetWOAgun How much proof does America AND th...\n",
       "2   Wujudkan Masyarakat Yang Sehat dan Produktif D...\n",
       "3   https://t.co/2vo3iF4r2h The Unforgivable Reque...\n",
       "4   @MariaBiddle9 @LeahLschkupz @BoSnerdley Well t...\n",
       "..                                                ...\n",
       "94  @DollyD2022 The covid Vax causes spires in the...\n",
       "95  âœ… Moderate Republican \\nâœ… Female\\nâœ… White\\nâœ… P...\n",
       "96  @Den_1er Ni pour Poutine ni pour Zelensky et i...\n",
       "97  @CovergirlChase @frandrescher @RockmondDunbar ...\n",
       "98  Anggaran untuk perbaikan RPTRA tahun ini sudah...\n",
       "\n",
       "[99 rows x 1 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a452c477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@abcnews Without MSM including ABC 24/7 spewin...</td>\n",
       "      <td>Without MSM including ABC spewing propaganda a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://t.co/hbMth4sgVT</td>\n",
       "      <td>https hbMth sgVT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Erectile Dysfunction Almost 6x More Likely Aft...</td>\n",
       "      <td>Erectile Dysfunction Almost More Likely After ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#NeverForget #NoAmnesty for what they did duri...</td>\n",
       "      <td>NeverForget NoAmnesty for what they did during...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://t.co/9bpfvWMRiZ Susah bernafas serta d...</td>\n",
       "      <td>https bpfvWMRiZ Susah bernafas serta dada tera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Production Hit At Worldâ€™s Largest iPhone Facto...</td>\n",
       "      <td>Production Hit World Largest iPhone Factory Af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>@JordanSchachtel ALL official truths are false...</td>\n",
       "      <td>ALL official truths are false Moon landing Ken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>@AquiAnitters @dayligharcher @felipeneto AÃ­ o ...</td>\n",
       "      <td>risco cada pessoa vai avaliar hora fazer meme ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>\"Unhinged hysteria\" is the key. Think COVID di...</td>\n",
       "      <td>Unhinged hysteria the key Think COVID discours...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Embarrassing that Stitt used tens of thousands...</td>\n",
       "      <td>Embarrassing that Stitt used tens thousands do...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              content  \\\n",
       "0   @abcnews Without MSM including ABC 24/7 spewin...   \n",
       "1                             https://t.co/hbMth4sgVT   \n",
       "2   Erectile Dysfunction Almost 6x More Likely Aft...   \n",
       "3   #NeverForget #NoAmnesty for what they did duri...   \n",
       "4   https://t.co/9bpfvWMRiZ Susah bernafas serta d...   \n",
       "..                                                ...   \n",
       "94  Production Hit At Worldâ€™s Largest iPhone Facto...   \n",
       "95  @JordanSchachtel ALL official truths are false...   \n",
       "96  @AquiAnitters @dayligharcher @felipeneto AÃ­ o ...   \n",
       "97  \"Unhinged hysteria\" is the key. Think COVID di...   \n",
       "98  Embarrassing that Stitt used tens of thousands...   \n",
       "\n",
       "                                            processed  \n",
       "0   Without MSM including ABC spewing propaganda a...  \n",
       "1                                    https hbMth sgVT  \n",
       "2   Erectile Dysfunction Almost More Likely After ...  \n",
       "3   NeverForget NoAmnesty for what they did during...  \n",
       "4   https bpfvWMRiZ Susah bernafas serta dada tera...  \n",
       "..                                                ...  \n",
       "94  Production Hit World Largest iPhone Factory Af...  \n",
       "95  ALL official truths are false Moon landing Ken...  \n",
       "96  risco cada pessoa vai avaliar hora fazer meme ...  \n",
       "97  Unhinged hysteria the key Think COVID discours...  \n",
       "98  Embarrassing that Stitt used tens thousands do...  \n",
       "\n",
       "[99 rows x 2 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get processed tweet\n",
    "tweet_df_processed = preprocess(tweet_df)\n",
    "tweet_df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a63d776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
